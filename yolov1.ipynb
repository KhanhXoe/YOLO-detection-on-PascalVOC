{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from matplotlib import patches\n",
    "from PIL import Image\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Module\n",
    "from torch.nn import functional as f\n",
    "from torch.nn.modules.activation import Sigmoid\n",
    "from torch.nn.modules.conv import Conv2d\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import utils as Vision_utils\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transforms = None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        boxes = list()\n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                class_label, x, y, w, h = [\n",
    "                    float(x) if float(x) != int (float(x)) \n",
    "                            else int(x) for x in label.replace(\"\\n\", \"\").split()\n",
    "                ]\n",
    "                boxes.append([class_label, x, y, w, h])\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path)\n",
    "        boxes = torch.tensor(boxes)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, boxes = self.transforms(image, boxes)\n",
    "        \n",
    "        label_matrix = torch.zeros((self.S, self.S, self.B*5 + self.C))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, w, h = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            #i, j biểu diễn cho chỉ số hàng và cột của ô lưới chứa boundbox \n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S *y - i\n",
    "            w_cell, h_cell = (w * self.S, h * self.S)\n",
    "\n",
    "            if label_matrix[i, j, 20] == 0:\n",
    "                label_matrix[i, j, 20] = 1\n",
    "\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, w_cell, h_cell]\n",
    "                )\n",
    "                label_matrix[i, j, 21:25] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "        \n",
    "        return image, label_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format= \"midpoint\"):\n",
    "    \"\"\"\n",
    "    Tính toán mức độ chồng lấn giữa các boxes giới hạn\n",
    "    Input: \n",
    "        boxes_preds (tensor): Dự đoán về các box giới hạn (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Nhãn chính xác của các box giới hạn (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, nếu box giới hạn được biểu diễn dưới dạng (x,y,w,h) hoặc (x1,y1,x2,y2)\n",
    "    Returns:\n",
    "        tensor: Độ chồng lấn giữa các ví dụ\n",
    "    \"\"\"\n",
    "    # Kiểm tra định dạng: \"midpoint\"\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[:,0] - boxes_preds[:, 2]/2\n",
    "        box1_y1 = boxes_preds[:,1] - boxes_preds[:, 4]/2\n",
    "        box1_x2 = boxes_preds[:,1] + boxes_preds[:, 2]/2\n",
    "        box1_y2 = boxes_preds[:,1] + boxes_preds[:, 4]/2\n",
    "\n",
    "        box2_x1 = boxes_labels[:,0] - boxes_labels[:, 2]/2\n",
    "        box2_y1 = boxes_labels[:,1] - boxes_labels[:, 4]/2\n",
    "        box2_x2 = boxes_labels[:,0] + boxes_labels[:, 2]/2\n",
    "        box2_y2 = boxes_labels[:,1] + boxes_labels[:, 4]/2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        # Trích xuất tọa độ cho các box dự đoán\n",
    "        box1_x1 = boxes_preds[:, 0]\n",
    "        box1_y1 = boxes_preds[:, 1]\n",
    "        box1_x2 = boxes_preds[:, 2]\n",
    "        box1_y2 = boxes_preds[:, 3]\n",
    "\n",
    "        # Trích xuất tọa độ cho nhãn thực\n",
    "        box2_x1 = boxes_labels[:, 0]\n",
    "        box2_y1 = boxes_labels[:, 1]\n",
    "        box2_x2 = boxes_labels[:, 2]\n",
    "        box2_y2 = boxes_labels[:, 3]\n",
    "\n",
    "        x1 = torch.max(box1_x1, box2_x1)\n",
    "        y1 = torch.max(box1_y1, box2_y1)\n",
    "        x2 = torch.min(box1_x2, box2_x2)\n",
    "        y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "        intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "        box1_area = abs((box1_x1-box1_x2) * (box1_y1-box1_y2))\n",
    "        box2_area = abs((box2_x1-box2_x2) * (box2_y1-box2_y2))\n",
    "\n",
    "        return intersection/(box1_area + box2_area - intersection + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_maximum_suppression(bboxes, iou_threshold, threshold, box_format= \"corners\"):\n",
    "    \"\"\"\n",
    "    Thực hiện Non-Maximum Suppression trên danh sách các box giới hạn\n",
    "    Parameters:\n",
    "        bboxes (list): Danh sách các box giới hạn, mỗi box được biểu diễn như [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        iou_threshold (float): Ngưỡng IoU để xác định box giới hạn dự đoán là đúng\n",
    "        threshold (float): Ngưỡng để loại bỏ các box giới hạn dự đoán (không phụ thuộc vào IoU)\n",
    "        box_format (str): \"midpoint\" hoặc \"corners\" để chỉ định định dạng của box giới hạn\n",
    "    Returns:\n",
    "        list: Danh sách các box giới hạn sau khi thực hiện NMS với một ngưỡng IoU cụ thể\n",
    "    \"\"\"\n",
    "    assert type(bboxes) == list\n",
    "    \n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key= lambda x: x[1], reverse= True)\n",
    "\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "        bboxes = [\n",
    "            box for box in bboxes\n",
    "            if box[0] != chosen_box[0] or \n",
    "            intersection_over_union(\n",
    "                torch.tensor(chosen_box[:, 2:], \n",
    "                torch.tensor(box[:,2:])),\n",
    "                box_format= box_format\n",
    "                ) < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "    \n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precison(\n",
    "        pred_boxes, true_boxes, iou_threshold= 0.6, \n",
    "        box_format= 'midpoint', num_classes= 20\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Tính giá trị trung bình mAP\n",
    "    Parameters:\n",
    "        pred_boxes (list): Danh sách chứa các boxes giới hạn dự đoán với mỗi box được chỉ...\n",
    "        ...định như [train_idx, class_prediction, prob_score, x1, y1, x2, y2].\n",
    "        true_boxes (list): Tương tự với pred_boxes với định dạng tương tự.\n",
    "        iou_threshold: ngưỡng IoU, nơi các box dự đoán được xem là đúng.\n",
    "        box_format (str): 'midpoint' & 'corner' được sử dụng để chỉ định định dạng box.\n",
    "        num_classes: Số lượng lớp.\n",
    "    Return: \n",
    "        float: Giá trị mAP qua tất cả các lớp với một ngưỡng IoU cụ thể\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "        # Lọc qua các dự đoán đối với phân lớp hiện tại\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        \n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "        \n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        for key, val in amount_bboxes.item():\n",
    "            amount_bboxes[key] = torch.tensor(val) # Số lượng boxes cho mỗi class huấn luyện\n",
    "        \n",
    "        detections.sort(key= lambda x: x[2], reverse= True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_boxes = len(ground_truths)\n",
    "\n",
    "        if total_true_boxes==0: continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format= box_format\n",
    "                )\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_iou_idx = idx\n",
    "            \n",
    "            if best_iou > iou_threshold:\n",
    "                if amount_bboxes[detection[0]][best_iou_idx] == 0: \n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_iou_idx] = 1\n",
    "                else: FP[detection_idx] = 1\n",
    "            \n",
    "            else: FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim= 0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim= 0)\n",
    "        recalls = TP_cumsum/(total_true_boxes + epsilon)\n",
    "        precisions = torch.devide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz để tích hợp số\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes):\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape()\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(im)\n",
    "\n",
    "    for box in boxes:\n",
    "        # |box[2:]| = [x_ctr, y_ctr, width, height]\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Có nhiều hơn x, y, w, h trong 1 box\"\n",
    "        upper_left_x = box[0] - box[2]/2\n",
    "        upper_left_y = box[1] - box[3]/2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height), \n",
    "            width= box[2]*width, height= box[3]*height, \n",
    "            linewidth = 1,\n",
    "            edgecolor= 'r', facecolor= 'none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(loader, model, iou_threshold, \n",
    "               threshold, pred_format= 'cells', \n",
    "               box_format= 'midpoint', device='cude'):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicts = model(x)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_maximum_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold = iou_threshold,\n",
    "                threshold = threshold,\n",
    "                box_format = box_format,\n",
    "            )\n",
    "\n",
    "            if batch_idx == 0 and idx == 0:\n",
    "                plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "                print(nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=7):\n",
    "    \"\"\"\n",
    "    Chuyển đổi các hộp giới hạn được đầu ra từ YOLO với kích thước chia ảnh S...\n",
    "    ...thành tỷ lệ của toàn bộ ảnh thay vì tỷ lệ của ô.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, 30)\n",
    "    \n",
    "    bboxes1 = predictions[..., 21:25]\n",
    "    bboxes2 = predictions[..., 26:30]\n",
    "    \n",
    "    scores = torch.cat(\n",
    "        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    \n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "    \n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    \n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(-1)\n",
    "    \n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes\n",
    "\n",
    "# Lưu trạng thái của mô hình và bộ tối ưu vào một tệp checkpoint. \n",
    "# Mặc định là \"checkpoint_model.tar\".\n",
    "def save_checkpoint(state, filename=\"checkpoint_model.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "# Tải trạng thái của mô hình và bộ tối ưu từ một checkpoint đã được lưu trước đó.\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mô tả các lớp convolution và max pooling, số lần lặp lại các khối convolution.\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias= False, **kwargs)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.batch_norm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1(nn.Module):\n",
    "    def __init__(self, in_channels= 3, **kwargs):\n",
    "        super(YOLOv1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.dark_net = self._create_conv_layer(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dark_net(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim= 1))\n",
    "    \n",
    "    def _create_conv_layer(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(in_channels, x[1], kernel_size= x[0], stride= x[2], padding= x[3])\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size= (2,2), stride= (2,2))]\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                repeats = x[2]\n",
    "\n",
    "                for _ in range(repeats):\n",
    "                    layers += [\n",
    "                        CNNBlock(in_channels, conv1[1], \n",
    "                                 kernel_size= conv1[0], \n",
    "                                 stride= conv1[2], \n",
    "                                 padding= conv1[3])\n",
    "                    ]\n",
    "                    layers += [\n",
    "                        CNNBlock(conv1[1], \n",
    "                                 conv2[1], \n",
    "                                 kernel_size= conv2[0], \n",
    "                                 stride= conv2[2], \n",
    "                                 padding= conv2[3])\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    def _create_fcs(self, S, B, C):\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 496), \n",
    "            nn.Dropout(0.1), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(496, S*S*(B*5+C))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_Loss(nn.Module):\n",
    "    def __init__(self, S=7, B= 2, C= 20):\n",
    "        super(YOLO_Loss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        predictions = predictions.reshape(-1, self.S*self.S*(self.B*5+self.C))\n",
    "\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], axis= 0)\n",
    "\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 20].unsqueeze(3)\n",
    "\n",
    "        box_predictions = exists_box * (\n",
    "            bestbox * predictions[..., 26:30]\n",
    "            + (1 - bestbox) * predictions[..., 21:25]\n",
    "        )\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "        \n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 20:21]),\n",
    "        )\n",
    "\n",
    "        #max_no_obj = torch.max(dự đoán[..., 20:21], dự đoán[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        # torch.flatten((1 - tồn tại_box) * max_no_obj, start_dim=1),\n",
    "        # torch.flatten((1 - tồn tại_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n",
    "        )\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss + object_loss + self.lambda_noobj * no_object_loss + class_loss\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
